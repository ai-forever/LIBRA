[parameters]
datasets = ["all"]
context_lengths = ["4k", "8k", "16k", "32k"]
max_context_length = 32768
model_path = lmsys/longchat-7b-v1.5-32k
tokenizer_path = meta-llama/Llama-2-7b-chat-hf
device = cuda
model_torch_dtype = bfloat16
save_path = predictions/longchat32k.json
chat_model = True
sys_prompt = You are a helpful assistant.
