[parameters]
datasets = ["all"]
context_lengths = ["4k", "8k", "16k", "32k", "64k", "128k" ]
max_context_length = 128000
model_path = mistralai/Mistral-Nemo-Instruct-2407
tokenizer_path = mistralai/Mistral-Nemo-Instruct-2407
device = cuda
model_torch_dtype = bfloat16
save_path = predictions/mistral_nemo_it_2407.json
chat_model = True
sys_prompt = You are a helpful assistant.
engine = vllm
tensor_parallel_size = 8
