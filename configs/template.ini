[parameters]
datasets = ["all"] or ["passkey", "matreshka_names", ...]
context_lengths = Choose one or all of them: ["4k", "8k", "16k", "32k", "64k", "128k"]
max_context_length = max context length of the model (for example, 32768)
model_path = local or HF path to the model
tokenizer_path = local or HF path to the tokenizer
device = cpu or cuda (cuda:0 or cuda:1 are also possible) or auto
model_torch_dtype = bfloat16 or float16 or float32
save_path = Path to save predictions (for example, predictions/chatglm2-6b-32k.json)
chat_model = Use tokenizer.apple_chat_template(). True or False. Optional parameter.
sys_prompt = Specific system prompt. Optional parameter.
