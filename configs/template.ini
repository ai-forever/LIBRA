[parameters]
engine = "hf" or "vllm"
tensor_parallel_size = only for vllm: the number of GPUs to use for distributed execution with tensor parallelism: [1, 2, .., 8]
datasets = ["all"] or ["passkey", "matreshka_names", ...]
context_lengths = chose one or all of them: ["4k", "8k", "16k", "32k", "64k", "128k"]
max_context_length = max context length of the model (for example, 32768)
model_path = local or HF path to the model
tokenizer_path = local or HF path to the tokenizer
device = cpu or cuda (cuda:0 or cuda:1 are also possible) or auto
model_torch_dtype = bfloat16 or float16 or float32
save_path = path to save predictions (for example, predictions/chatglm2-6b-32k.json)
