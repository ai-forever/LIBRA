[parameters]
datasets = ["all"]
context_lengths = ["4k", "8k"]
max_context_length = 8192
model_path = IlyaGusev/saiga_llama3_8b
tokenizer_path = IlyaGusev/saiga_llama3_8b
device = cuda
model_torch_dtype = bfloat16
save_path = predictions/saiga_llama3.json
